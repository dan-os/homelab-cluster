Name:         rook-ceph
Namespace:    rook-ceph
Labels:       app.kubernetes.io/managed-by=Helm
              helm.toolkit.fluxcd.io/name=rook-ceph-cluster
              helm.toolkit.fluxcd.io/namespace=rook-ceph
Annotations:  meta.helm.sh/release-name: rook-ceph-cluster
              meta.helm.sh/release-namespace: rook-ceph
API Version:  ceph.rook.io/v1
Kind:         CephCluster
Metadata:
  Creation Timestamp:             2023-04-08T23:27:03Z
  Deletion Grace Period Seconds:  0
  Deletion Timestamp:             2023-04-08T23:32:04Z
  Finalizers:
    cephcluster.ceph.rook.io
  Generation:  5
  Managed Fields:
    API Version:  ceph.rook.io/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:finalizers:
          .:
          v:"cephcluster.ceph.rook.io":
      f:spec:
        f:external:
        f:healthCheck:
          f:daemonHealth:
            f:osd:
              f:interval:
            f:status:
              f:interval:
        f:network:
          f:multiClusterService:
        f:resources:
          f:mgr:
            f:limits:
              f:cpu:
          f:mon:
            f:limits:
              f:cpu:
            f:requests:
              f:cpu:
          f:osd:
            f:limits:
              f:cpu:
            f:requests:
              f:cpu:
        f:security:
          .:
          f:keyRotation:
            .:
            f:enabled:
          f:kms:
        f:storage:
          f:nodes:
    Manager:      rook
    Operation:    Update
    Time:         2023-04-08T23:27:03Z
    API Version:  ceph.rook.io/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:meta.helm.sh/release-name:
          f:meta.helm.sh/release-namespace:
        f:labels:
          .:
          f:app.kubernetes.io/managed-by:
          f:helm.toolkit.fluxcd.io/name:
          f:helm.toolkit.fluxcd.io/namespace:
      f:spec:
        .:
        f:cephVersion:
          .:
          f:image:
        f:cleanupPolicy:
          .:
          f:sanitizeDisks:
            .:
            f:dataSource:
            f:iteration:
            f:method:
        f:crashCollector:
        f:dashboard:
          .:
          f:enabled:
          f:ssl:
          f:urlPrefix:
        f:dataDirHostPath:
        f:disruptionManagement:
          .:
          f:managePodBudgets:
          f:osdMaintenanceTimeout:
        f:healthCheck:
          .:
          f:daemonHealth:
            .:
            f:mon:
              .:
              f:interval:
            f:osd:
            f:status:
          f:livenessProbe:
            .:
            f:mgr:
            f:mon:
            f:osd:
        f:logCollector:
          .:
          f:enabled:
          f:maxLogSize:
          f:periodicity:
        f:mgr:
          .:
          f:count:
          f:modules:
        f:mon:
          .:
          f:count:
        f:monitoring:
        f:network:
          .:
          f:connections:
            .:
            f:compression:
              .:
              f:enabled:
            f:encryption:
          f:provider:
        f:priorityClassNames:
          .:
          f:mgr:
          f:mon:
          f:osd:
        f:resources:
          .:
          f:cleanup:
            .:
            f:limits:
              .:
              f:cpu:
              f:memory:
            f:requests:
              .:
              f:cpu:
              f:memory:
          f:crashcollector:
            .:
            f:limits:
              .:
              f:cpu:
              f:memory:
            f:requests:
              .:
              f:cpu:
              f:memory:
          f:logcollector:
            .:
            f:limits:
              .:
              f:cpu:
              f:memory:
            f:requests:
              .:
              f:cpu:
              f:memory:
          f:mgr:
            .:
            f:limits:
              .:
              f:memory:
            f:requests:
              .:
              f:cpu:
              f:memory:
          f:mgr-sidecar:
            .:
            f:limits:
              .:
              f:cpu:
              f:memory:
            f:requests:
              .:
              f:cpu:
              f:memory:
          f:mon:
            .:
            f:limits:
              .:
              f:memory:
            f:requests:
              .:
              f:memory:
          f:osd:
            .:
            f:limits:
              .:
              f:memory:
            f:requests:
              .:
              f:memory:
          f:prepareosd:
            .:
            f:requests:
              .:
              f:cpu:
              f:memory:
        f:storage:
          .:
          f:config:
            .:
            f:osdsPerDevice:
          f:useAllDevices:
        f:waitTimeoutForHealthyOSDInMinutes:
    Manager:      helm-controller
    Operation:    Update
    Time:         2023-04-09T00:25:26Z
    API Version:  ceph.rook.io/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:status:
        .:
        f:conditions:
        f:message:
        f:phase:
        f:state:
        f:version:
          .:
          f:image:
          f:version:
    Manager:      rook
    Operation:    Update
    Subresource:  status
    Time:         2023-04-09T13:43:34Z
    API Version:  ceph.rook.io/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:spec:
        f:cleanupPolicy:
          f:confirmation:
    Manager:         kubectl-patch
    Operation:       Update
    Time:            2023-04-09T14:53:21Z
  Resource Version:  1335868
  UID:               beba07ff-a05e-41ee-8369-bb243933d3c1
Spec:
  Ceph Version:
    Image:  quay.io/ceph/ceph:v17.2.5
  Cleanup Policy:
    Confirmation:  yes-really-destroy-data
    Sanitize Disks:
      Data Source:  zero
      Iteration:    1
      Method:       quick
  Crash Collector:
  Dashboard:
    Enabled:           true
    Ssl:               false
    URL Prefix:        /
  Data Dir Host Path:  /var/lib/rook
  Disruption Management:
    Manage Pod Budgets:       true
    Osd Maintenance Timeout:  30
  External:
  Health Check:
    Daemon Health:
      Mon:
        Interval:  45s
      Osd:
        Interval:  1m0s
      Status:
        Interval:  1m0s
    Liveness Probe:
      Mgr:
      Mon:
      Osd:
  Log Collector:
    Enabled:       true
    Max Log Size:  500M
    Periodicity:   daily
  Mgr:
    Count:  2
    Modules:
      Enabled:  true
      Name:     pg_autoscaler
  Mon:
    Count:  3
  Monitoring:
  Network:
    Connections:
      Compression:
        Enabled:  true
      Encryption:
    Multi Cluster Service:
    Provider:  host
  Priority Class Names:
    Mgr:  system-cluster-critical
    Mon:  system-node-critical
    Osd:  system-node-critical
  Resources:
    Cleanup:
      Limits:
        Cpu:     500m
        Memory:  1Gi
      Requests:
        Cpu:     500m
        Memory:  100Mi
    Crashcollector:
      Limits:
        Cpu:     500m
        Memory:  60Mi
      Requests:
        Cpu:     100m
        Memory:  60Mi
    Logcollector:
      Limits:
        Cpu:     500m
        Memory:  1Gi
      Requests:
        Cpu:     100m
        Memory:  100Mi
    Mgr:
      Limits:
        Cpu:     1
        Memory:  1Gi
      Requests:
        Cpu:     500m
        Memory:  512Mi
    Mgr - Sidecar:
      Limits:
        Cpu:     500m
        Memory:  100Mi
      Requests:
        Cpu:     100m
        Memory:  40Mi
    Mon:
      Limits:
        Cpu:     2
        Memory:  2Gi
      Requests:
        Cpu:     1
        Memory:  1Gi
    Osd:
      Limits:
        Cpu:     2
        Memory:  4Gi
      Requests:
        Cpu:     1
        Memory:  4Gi
    Prepareosd:
      Requests:
        Cpu:     500m
        Memory:  50Mi
  Security:
    Key Rotation:
      Enabled:  false
    Kms:
  Storage:
    Config:
      Osds Per Device:  1
    Nodes:
      Devices:
        Name:  /dev/disk/by-id/wwn-0x6c81f660ecb783002bc470f922f352f4
      Name:    c0-aurora-0
      Resources:
      Devices:
        Name:  /dev/disk/by-id/wwn-0x6c81f660ecb783002bc4711524a168e5
      Name:    c0-aurora-1
      Resources:
      Devices:
        Name:  /dev/disk/by-id/wwn-0x55cd2e414e04bf67
      Name:    c0-ares-0
      Resources:
      Devices:
        Name:  /dev/disk/by-id/wwn-0x55cd2e414e04beca
      Name:    c0-ares-1
      Resources:
    Use All Devices:                        false
  Wait Timeout For Healthy OSD In Minutes:  10
Status:
  Conditions:
    Last Heartbeat Time:   2023-04-09T13:43:34Z
    Last Transition Time:  2023-04-08T23:32:06Z
    Message:               CephCluster "rook-ceph/rook-ceph" will not be deleted until all dependents are removed: CephBlockPool: [ceph-blockpool], CephFilesystem: [ceph-filesystem], CephObjectStore: [ceph-objectstore]
    Reason:                ObjectHasDependents
    Status:                True
    Type:                  DeletionIsBlocked
    Last Heartbeat Time:   2023-04-08T23:32:09Z
    Last Transition Time:  2023-04-08T23:32:09Z
    Message:               failed to create cluster: failed to start ceph monitors: failed to assign pods to mons: failed to schedule mons
    Reason:                ClusterProgressing
    Status:                False
    Type:                  Progressing
    Last Heartbeat Time:   2023-04-09T13:43:34Z
    Last Transition Time:  2023-04-08T23:32:06Z
    Message:               Deleting the CephCluster
    Reason:                ClusterDeleting
    Status:                True
    Type:                  Deleting
  Message:                 Deleting the CephCluster
  Phase:                   Deleting
  State:                   Deleting
  Version:
    Image:    quay.io/ceph/ceph:v17.2.5
    Version:  17.2.5-0
Events:       <none>
